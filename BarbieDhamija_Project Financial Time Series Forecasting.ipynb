{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8e9f56-05d4-4e22-b687-8151e3b2988c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from subprocess import run, PIPE\n",
    "import sys\n",
    "\n",
    "# List of modules to upgrade\n",
    "modules = [\n",
    "    'pandas_market_calendars',\n",
    "    'plotly', \n",
    "    'numpy',\n",
    "    'scikit-learn',\n",
    "    'pandas',\n",
    "    'matplotlib',\n",
    "    'mplfinance'\n",
    "]\n",
    "\n",
    "# Construct the pip install command\n",
    "command = f'pip install {\" \".join(modules)} --upgrade --user --no-input'\n",
    "\n",
    "# Run the command\n",
    "try:\n",
    "    proc = run(command, shell=True, text=True, stdout=PIPE, stderr=PIPE, timeout=120)\n",
    "\n",
    "    # Check if the installation was successful\n",
    "    if proc.returncode == 0:\n",
    "        print(\"All modules upgraded successfully.\")\n",
    "    else:\n",
    "        print(\"Error upgrading modules:\")\n",
    "        print(proc.stderr)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "# Manual Step: Please restart the Kernel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76d10c5f-db21-42ac-95fc-623fb9a2bf4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grade 1\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=pd.errors.SettingWithCopyWarning)\n",
    "# Download and extract the CSV file from the ZIP\n",
    "conversion_data_url = \"https://people.arcada.fi/~parland/hjd5_8amp_Gt3/EURUSD1m.zip\"\n",
    "conversion_data_df = pd.read_csv(conversion_data_url, compression='zip')\n",
    "\n",
    "# Coverting it to string\n",
    "conversion_data_df['Date'] = conversion_data_df['Date'].astype(str)\n",
    "conversion_data_df['Timestamp'] = conversion_data_df['Timestamp'].astype(str)\n",
    "\n",
    "# combine them and convert them into a single DatetimeIndex.\n",
    "conversion_data_df['Datetime'] = pd.to_datetime(conversion_data_df['Date'] + ' ' + conversion_data_df['Timestamp'])\n",
    "# Drop the original 'Date' and 'Time' columns as not needed anymore\n",
    "conversion_data_df.drop(columns=['Date', 'Timestamp'], inplace=True)\n",
    "\n",
    "# Set the 'Datetime' column as the index\n",
    "conversion_data_df.set_index('Datetime', inplace=True)\n",
    "\n",
    "# adding th shift column by shifting the Close value one step forward\n",
    "min_to_day_df = conversion_data_df.resample('D').agg({'Open': 'first','High' : 'max' , 'Low': 'min','Close': 'last', 'Volume': 'sum' })\n",
    "\n",
    "\n",
    "# Apply forward fill to handle missing values for Saturdays\n",
    "min_to_day_df['Open'] = min_to_day_df['Open'].ffill()\n",
    "min_to_day_df['High'] = min_to_day_df['High'].ffill()\n",
    "min_to_day_df['Low'] = min_to_day_df['Low'].ffill()\n",
    "min_to_day_df['Close'] = min_to_day_df['Close'].ffill()\n",
    "min_to_day_df['Volume'] = min_to_day_df['Volume'].ffill()\n",
    "\n",
    "# copying the entire raw dataframe to a new dataframe for week ahead data calculation \n",
    "one_week_ahead_df = min_to_day_df.copy()\n",
    "\n",
    "# Creating a 'Label' column for the forecast by shifting the Close value one step forward.\n",
    "min_to_day_df['Label'] = min_to_day_df['Close'].shift(-1)\n",
    "min_to_day_df.dropna(subset=['Label'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3184547c-26c3-4f7f-84ae-3487529f20e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a split function for splitting the data in training and test : 80 20\n",
    "def split_data(data_df):\n",
    "    total_df_length  = len(data_df)\n",
    "    train_data_len = int(total_df_length*0.8)\n",
    "    test_data_len = total_df_length - train_data_len\n",
    "    train_data = data_df.iloc[: train_data_len]\n",
    "    test_data = data_df.iloc[train_data_len :]\n",
    "    print(f'{total_df_length} {train_data_len} {test_data_len}')\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bcb681a9-2d57-4fb2-9ae0-e99f7b69d43e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3651 2920 731\n"
     ]
    }
   ],
   "source": [
    "# Storing Actual Close and 1 day ahead Close data in separate dataframe for calculating Hit Ratio\n",
    "HR_calc_df = min_to_day_df[['Close','Label']].copy()\n",
    "# Drop rows where 'Label' is NaN (due to the shift)\n",
    "HR_calc_df.dropna(subset=['Label'], inplace=True)\n",
    "HR_calc_df.dropna(subset=['Close'], inplace=True)\n",
    "one_day_train_df, one_day_test_df = split_data(HR_calc_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94084054-308d-4b93-80a0-e47c1b1352ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing Actual Close and 7 days ahead Close data in separate dataframe for calculating Hit Ratio\n",
    "HR_calc_week_df = one_week_ahead_df[['Close']].copy()\n",
    "# Create a new column for Label, shifting the Close values 7 days ahead\n",
    "HR_calc_week_df['Label'] = HR_calc_week_df['Close'].shift(-7)\n",
    "HR_calc_week_df.dropna(subset=['Label'], inplace=True)\n",
    "HR_calc_week_df.dropna(subset=['Close'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a494b35d-cb82-42e3-b3ad-c27122fcc95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating Larry William’s %R\n",
    "window_size = 14\n",
    "min_to_day_df['highest_high'] = min_to_day_df['High'].rolling(window = window_size).max()\n",
    "min_to_day_df['lowest_low'] = min_to_day_df['Low'].rolling(window = window_size).min()\n",
    "min_to_day_df['%R'] = (min_to_day_df['highest_high']-min_to_day_df['Close'])/(min_to_day_df['highest_high']-min_to_day_df['lowest_low'])*-100\n",
    "min_to_day_df.drop(columns=['highest_high', 'lowest_low'], inplace=True)\n",
    "min_to_day_df = min_to_day_df.dropna(subset=['%R'])\n",
    "min_to_day_df = min_to_day_df.dropna(subset=['Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478efdb8-0a2a-4ca1-a071-58bcf5177ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling the Split function min_to_day_df (which has one day forecast label column) to split in training and test\n",
    "train_data_df, test_data_df = split_data(min_to_day_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be6d4873-a427-4793-99dc-01a02a6fce37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Defining a function to standardize the splitted data : train and test & return the standardized train and test dataframe\n",
    "# Note: this function will not standardize/transform the label column\n",
    "def standardized_train_test_data(train_dataset, test_dataset):\n",
    "    if '%R' in train_dataset.columns and '%R' in test_dataset.columns:\n",
    "        # Separate features and labels for train\n",
    "        train_feature_df = train_dataset[['Close', '%R']].copy()  \n",
    "        test_feature_df = test_dataset[['Close', '%R']].copy()\n",
    "       \n",
    "    else:\n",
    "        train_feature_df = train_dataset[['Close']].copy()\n",
    "        test_feature_df = test_dataset[['Close']].copy()\n",
    "    \n",
    "    # Separate features and labels for test data\n",
    "    test_label_df = test_dataset['Label']\n",
    "    train_label_df = train_dataset['Label']\n",
    "\n",
    "    # Instantiate the StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # Fit the scaler on the training data and transform it\n",
    "    train_scaled_array = scaler.fit_transform(train_feature_df)\n",
    "    # Convert scaled numpy arrays back to DataFrames\n",
    "    train_scaled_df = pd.DataFrame(train_scaled_array, columns=train_feature_df.columns, index=train_feature_df.index)\n",
    "    \n",
    "    # Transform the test data using the same scaler\n",
    "    test_scaled_array = scaler.transform(test_feature_df)\n",
    "    # Convert scaled numpy arrays back to DataFrames\n",
    "    test_scaled_df = pd.DataFrame(test_scaled_array, columns=test_feature_df.columns, index=test_feature_df.index)\n",
    "    \n",
    "    # Reattach the labels to the scaled features as Label is the target column, so it's not standardized\n",
    "    train_final_df = train_scaled_df.copy()\n",
    "    train_final_df['Label'] = train_label_df\n",
    "\n",
    "    test_final_df = test_scaled_df.copy()\n",
    "    test_final_df['Label'] = test_label_df\n",
    "\n",
    "    pd.set_option('display.float_format', '{:.6f}'.format)\n",
    "    return train_final_df, test_final_df,scaler\n",
    "\n",
    "# calling the function and saving the standardized train and test dataframes\n",
    "train_final_df, test_final_df, scaling_param = standardized_train_test_data(train_data_df, test_data_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b9b4ee-1c08-427c-96d7-0a608eb13e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Defining a function to fit and run : LinearRegression Model\n",
    "# This model will return the label, model predicted value, df: actual and predicted close value and R² for test and training\n",
    "def train_and_evaluate_model(train_data, test_data):\n",
    "    # Initialize the model based on the chosen type\n",
    "    if '%R' in train_data.columns and '%R' in test_data.columns:\n",
    "        X_train = train_data[['Close', '%R']]\n",
    "        y_train = train_data['Label']\n",
    "        X_test = test_data[['Close', '%R']]\n",
    "        y_test = test_data['Label']\n",
    "    else:\n",
    "        X_train = train_data[['Close']]\n",
    "        y_train = train_data['Label']\n",
    "        X_test = test_data[['Close']]\n",
    "        y_test = test_data['Label']\n",
    "    model = LinearRegression()    \n",
    "    # Fit the model on the training data\n",
    "    model.fit(X_train, y_train)\n",
    "    # Make predictions on the test data\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    # Make predictions on the train data\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    # Calculate R² score/ R2 Error on test\n",
    "    r2_test = r2_score(y_test, y_test_pred)\n",
    "    # Calculate R² score on train\n",
    "    r2_train = r2_score(y_train,y_train_pred)\n",
    "\n",
    "    results_df = pd.DataFrame({\n",
    "        'Actual': y_test,\n",
    "        'Predicted': y_test_pred\n",
    "    })\n",
    "    return r2_train,r2_test,results_df, y_test, y_test_pred\n",
    "\n",
    "# Train and evaluate the Linear Regression model\n",
    "r2_train , r2_test, comparison , y_test, y_test_pred = train_and_evaluate_model(train_final_df, test_final_df)\n",
    "print(f'R2 score  on test data is: {r2_test:.4f}')\n",
    "print(f'R2 score  on train data is: {r2_train:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd738e1c-7447-40e6-8953-efd8c4ef5738",
   "metadata": {},
   "source": [
    "### Grade 1:\n",
    "### 1.Compare the R² errors for test and train and explain the outcome.\n",
    "### Explanation:\n",
    "- **Training R² (0.9970)**: This means that the model can explain 99.70% of the variation in the training data. The model has learned the relationships between the inputs (Close and %R) and the target (Label) very well for the data it was trained on.\n",
    "- **Test R² (0.9913)**: This means that when I tested the model on new, unseen data, it can still explain 99.13% of the variation. This means that the model can predict well on new data it hasn't seen before, which is the ultimate goal.\n",
    "Key points:\n",
    "1. It's normal for the training R² to be slightly higher than the test R² because:\n",
    "    - The model has already seen the training data and learned from it, so it fits that data very well.\n",
    "    - The test data is new to the model, and since it hasn't encountered this data before, it might not fit it perfectly.\n",
    "2. However, in this case, the test R² is still very close to the training R², which is a positive sign!\n",
    "3. The small difference between the two R² scores (0.9970 vs. 0.9913) indicates that the model isn’t overfitting and is making accurate predictions on both the training and test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "061d300e-819e-4d02-9cf4-5f330a032e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding column for LinearRegression model predicted values\n",
    "one_day_test_df['Predicted'] = comparison['Predicted'].copy()\n",
    "one_day_test_df.dropna(subset=['Predicted'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f2feb7-788f-4c1c-a076-972e0d8a0f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Dropping the %R column from the standardized data for train and test (label not stanardized) to \n",
    "train_final_df = train_final_df.drop(columns=['%R'], errors='ignore')\n",
    "test_final_df = test_final_df.drop(columns=['%R'], errors='ignore')\n",
    "\n",
    "# Train and evaluate the Linear Regression model without %R\n",
    "r2_train , r2_test, Actual_Predicted_df, y_test, y_test_pred = train_and_evaluate_model(train_final_df, test_final_df)\n",
    "print(f'R2 score  on test data is: {r2_test:.4f}')\n",
    "print(f'R2 score  on train data is: {r2_train:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f4d8c3-b8f8-4da4-8bf3-f6742f40acdd",
   "metadata": {},
   "source": [
    "### Extra: Test your model (get R² errors for test and train without LW%R, just Close column). Comment and explain the result.\n",
    "### Explanation\n",
    "The R² scores remained unchanged after removing `%R` because the correlation between `Close` and `%R` is very weak, indicating that `%R` doesn’t add\n",
    "significant value to the model. Since `%R` is not providing additional predictive power, the `Close` value alone captures most of the necessary \n",
    "information for accurate predictions. The model heavily relies on `Close`, which is already a strong predictor. Consequently, `%R` has minimal impact \n",
    "on the model’s performance, so its removal doesn’t affect the R² scores. This explains why both the training and test R² scores remained high and \n",
    "consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05841b92-5b7b-4c58-929e-2efaf0089562",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Below graph shows the LinerarRegression model one day ahead predicted and actual close value\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(y_test.index, y_test, label='Actual Values', color='blue')\n",
    "plt.plot(y_test.index, y_test_pred, label='Predicted Values', color='yellow')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Close Value')\n",
    "plt.title('Actual vs. Predicted Close Values')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef3f8eb2-98eb-4a13-b358-f1dfc2d3b93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grade 2\n",
    "# calculate stochastic %D\n",
    "import pandas as pd\n",
    "\n",
    "# Define the window sizes\n",
    "window_size_k = 14  # Period for %K calculation\n",
    "window_size_d = 3   # Period for smoothing %K to get Slow %K and Slow %D\n",
    "\n",
    "# Calculate %K\n",
    "min_to_day_df['highest_high'] = min_to_day_df['High'].rolling(window=window_size_k, min_periods=1).max()\n",
    "min_to_day_df['lowest_low'] = min_to_day_df['Low'].rolling(window=window_size_k, min_periods=1).min()\n",
    "min_to_day_df['%K'] = (min_to_day_df['Close'] - min_to_day_df['lowest_low']) / (min_to_day_df['highest_high'] - min_to_day_df['lowest_low']) * 100\n",
    "\n",
    "# Drop temporary columns used for calculation\n",
    "min_to_day_df.drop(columns=['highest_high', 'lowest_low'], inplace=True)\n",
    "\n",
    "# Calculate Slow %K by applying a moving average to %K\n",
    "min_to_day_df['Slow %K'] = min_to_day_df['%K'].rolling(window=window_size_d, min_periods=1).mean()\n",
    "\n",
    "# Calculate Slow %D by applying a moving average to Slow %K\n",
    "min_to_day_df['Slow %D'] = min_to_day_df['Slow %K'].rolling(window=window_size_d, min_periods=1).mean()\n",
    "\n",
    "# Drop rows with NaN values if any (usually at the start of the series)\n",
    "min_to_day_df.dropna(subset=['Slow %D'], inplace=True)\n",
    "\n",
    "min_to_day_df.drop(columns=['Slow %K' , '%K'], inplace=True)\n",
    "# Display the DataFrame with the new feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08dd4381-0caf-4287-b2a7-06060a853efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling split data function again for further calculation\n",
    "train_data_df, test_data_df = split_data(min_to_day_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "72a63162-afe9-466d-b16e-e021f856f3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding new columns in test dataframe for plotting purpose\n",
    "test_data_df['Predicted'] = y_test_pred  # Add predicted values to the DataFrame  \n",
    "test_data_df.rename(columns={'Label': 'Actual Data', 'Predicted': 'Forecast Data'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a98037d-6f19-46f3-b581-1bd58b0e33be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from mplfinance.original_flavor import candlestick_ohlc\n",
    "import pandas as pd\n",
    "\n",
    "# Creating few functions below for plotting: OHLC candle with Actual and Predicted close value, %R, Slow %D, Calculating and plotting RSI.\n",
    "\n",
    "# Function to plot OHLC chart\n",
    "def plot_ohlc(test_data_df):\n",
    "    ohlc_data = []\n",
    "    for i in range(len(test_data_df)):\n",
    "        date = mdates.date2num(test_data_df.index[i].to_pydatetime())  # Convert date to matplotlib date format\n",
    "        open_value = test_data_df['Open'].iloc[i]\n",
    "        high_value = test_data_df['High'].iloc[i]\n",
    "        low_value = test_data_df['Low'].iloc[i]\n",
    "        close_value = test_data_df['Close'].iloc[i]\n",
    "        ohlc_data.append((date, open_value, high_value, low_value, close_value))\n",
    "    return ohlc_data\n",
    "\n",
    "# This function will show the OHLC candles and Actual and predicted close value in the same figure\n",
    "def plot_ohlc_actual_vs_predicted(test_data_df):\n",
    "    # Use a limited view for clarity\n",
    "    recent_data = test_data_df.iloc[1:150]  # Only the first 150 entries for clearer view\n",
    "    fig, ax = plt.subplots(figsize=(16, 8))  # Increase figure size for better visibility\n",
    "\n",
    "    # Plot OHLC\n",
    "    ohlc_data = plot_ohlc(recent_data)\n",
    "    ax2 = ax.twinx()  # Create a twin axis for the candlestick chart\n",
    "    candlestick_ohlc(ax2, ohlc_data, width=0.2, colorup='g', colordown='r')  # Adjust candle width\n",
    "    ax2.xaxis_date()\n",
    "    ax2.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "\n",
    "    # Adjust y-limits to provide more space above and below, to see clarity in the graph\n",
    "    y_min = recent_data['Low'].min() * 1.00  # Lower limit with a margin\n",
    "    y_max = recent_data['High'].max() * 1.05  # Upper limit with a margin\n",
    "    ax2.set_ylim(y_min, y_max)\n",
    "\n",
    "    ax2.set_title('OHLC Candlestick Chart and Actual vs. Predicted Close Values')\n",
    "    ax2.set_ylabel('Value')\n",
    "    ax2.grid()\n",
    "\n",
    "    # Plot Actual vs. Predicted on the same axis\n",
    "    ax.plot(recent_data.index, recent_data['Actual Data'], label='Actual Close', color='blue', linewidth=2)\n",
    "    ax.plot(recent_data.index, recent_data['Forecast Data'], label='Predicted Close', color='orange', linestyle='--', linewidth=2)\n",
    "\n",
    "    ax.xaxis_date()\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "    ax.set_ylabel('Value')\n",
    "    ax.grid()\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# This function will show the %R and Slow %D \n",
    "def plot_r_and_d(test_data_df):\n",
    "    fig, ax = plt.subplots(figsize=(14, 7))\n",
    "    ax.plot(test_data_df.index, test_data_df['%R'], color='yellow', label='LW%R')\n",
    "    ax.plot(test_data_df.index, test_data_df['Slow %D'], color='purple', label='Slow %D')\n",
    "    ax.xaxis_date()\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "    ax.set_title('LW%R and Slow %D')\n",
    "    ax.set_ylabel('Indicator Values')\n",
    "    ax.grid()\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Function calling\n",
    "\n",
    "# Create a figure based on OHLC candles covering the test period (the 20% of data)\n",
    "# Add a line to the chart that illustrates the label (actual data) and the forecast (so candels and lines are in the same figure. \n",
    "plot_ohlc_actual_vs_predicted(test_data_df)\n",
    "\n",
    "# Add subplot(s) with the LW%R and Stochastic slow %D features\n",
    "plot_r_and_d(test_data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576dac2c-09c5-4b26-b1b6-38c8c2a99d5d",
   "metadata": {},
   "source": [
    "### Grade 2:\n",
    "### 1. What patterns can you observe from the line figure?\n",
    "### Explanation\n",
    "- **Two Lines: Actual(label: Actual one day ahead close value) and Predicted Close value:**\n",
    "The Actual and Predicted lines are very close to each other means the model is doing a good job of predicting the close value. When the blue line \n",
    "(actual value) moves up or down, the orange line (predicted value) also follows in the same direction.\n",
    "- There are some places where the orange dashed line does not match exactly with the blue line like around February 2018, there is a noticeable gap where the orange dashed line is higher than the blue line. The differences can be seen where the lines don't overlap exactly, but these are small and show the model was still close in predicting overall market trends.\n",
    "- **Candlestick Chart:**\n",
    "    - Below the two lines, there is a candlestick chart (green and red bars). Each candlestick represents the movement of value for a day.\n",
    "    - Green candles mean the value went up during a day.\n",
    "    - Red candles mean the value went down during that day.\n",
    "    - The candlestick chart generally follows the same up-and-down movement as the lines above. This shows that your model's predictions reflect the real changes happening in the market.\n",
    "- If we talk about the trend then both the actual and predicted lines show similar patterns. When the value is trending upwards (going up), the predicted line also goes up. When the value is trending downwards, the predicted line also follows the downtrend means model is good at predicting general trends\n",
    "in the market."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eab08db-5fce-4a35-81d9-78ea1a27a9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Function to calculate RSI\n",
    "def calculate_rsi(data):\n",
    "    window_size = 14\n",
    "    delta = data['Close'].diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=window_size).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=window_size).mean()\n",
    "    rs = gain / loss\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "    return rsi\n",
    "\n",
    "# Calculate additional feature RSI (relative strength index)\n",
    "test_data_df['RSI'] = calculate_rsi(test_data_df)\n",
    "\n",
    "# Function to plot RSI\n",
    "def plot_rsi(test_data_df, window=14):\n",
    "    # Calculate RSI\n",
    "    test_data_df['RSI'] = calculate_rsi(test_data_df)\n",
    "    fig, ax = plt.subplots(figsize=(14, 7))\n",
    "    ax.plot(test_data_df.index, test_data_df['RSI'], color='cyan', label='RSI')\n",
    "    ax.set_title('Relative Strength Index (RSI)')\n",
    "    ax.set_ylabel('RSI Value')\n",
    "    ax.xaxis_date()\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))  # Format dates\n",
    "    ax.grid()\n",
    "    ax.legend(['RSI'])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Add the feature as a subplot to the figure in the previos step\n",
    "plot_rsi(test_data_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d55bcb-6e63-44d2-8cec-6deadcdfe243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up an ElasticNet (not an ElasticNetCV) model\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "# Defining model to train and run: ElasticNet and calculate R² Error on both the training data set and the test\n",
    "def elasticNet_train_and_predict(train_data, test_data, alpha_param,l1_ratio_param):\n",
    "\n",
    "    X_train = train_data[['Close']]\n",
    "    y_train = train_data['Label']\n",
    "    X_test = test_data[['Close']]\n",
    "    y_test = test_data['Label']\n",
    "    # Initialize the ElasticNet model with regularization\n",
    "    elasticNet = ElasticNet(alpha=alpha_param, l1_ratio =l1_ratio_param)\n",
    "\n",
    "    # training the data on elasticNet model\n",
    "    elasticNet.fit(X_train, y_train)\n",
    "    # Make predictions\n",
    "    # y_pred is y_test_elastic_pred\n",
    "    y_test_elastic_p = elasticNet.predict(X_test) \n",
    "\n",
    "# Calculate R² score/ R2 Error on test\n",
    "    r_square_5 = r2_score(y_test, y_test_elastic_p)\n",
    "    print(f'R² score on test data: {r_square_5:.4f}')\n",
    "    y_train_elastic_pred = elasticNet.predict(X_train)\n",
    "\n",
    "# Calculate R² score on train\n",
    "    r_square_6 = r2_score(y_train,y_train_elastic_pred)\n",
    "    print(f'R² score on train data: {r_square_6:.4f}')\n",
    "    test_data_df['ElasticNet_Predicted'] = y_test_elastic_p\n",
    "    elastic_results_df = pd.DataFrame({\n",
    "        'Actual': y_test,\n",
    "        'Predicted': y_test_elastic_p\n",
    "    })\n",
    "    return y_test, y_test_elastic_p, elastic_results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "72ee6fe4-fbd0-44cf-a283-ec12da226f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot Actual and Predicted values\n",
    "def plot_actual_vs_predicted(test_data_df):\n",
    "    fig, ax = plt.subplots(figsize=(14, 7))\n",
    "    # Plot Actual vs. Predicted on the same axis\n",
    "    ax.plot(test_data_df.index, test_data_df['Actual Data'], label='Actual', color='blue')\n",
    "    ax.plot(test_data_df.index, test_data_df['Forecast Data'], label='Predicted', color='orange')\n",
    "    ax.plot(test_data_df.index, test_data_df['ElasticNet_Predicted'], label='ElasticNet Predicted', color='green')\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "    ax.set_ylabel('Value')\n",
    "    ax.grid()\n",
    "    ax.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035df20f-5437-4e54-bfdb-ab1f000a6cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The suitable values for alpha and l1_ratio which fits the model are alpha=0.01(relarization very low) and l1_ratio= 0.5\n",
    "y_test, y_test_elastic_p, elastic_results_df =elasticNet_train_and_predict(train_final_df, test_final_df, 0.01,0.5)\n",
    "\n",
    "#Plotting with alpha value too low (0.01)\n",
    "plot_actual_vs_predicted(test_data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0452bfd-c3d4-4e8a-826c-4314066fdbe6",
   "metadata": {},
   "source": [
    "### Grade 3:  \n",
    "### 1. Compare the errors and explain the outcome.\n",
    "### Explanation\n",
    "**Linear Regression R² Error on test is = 0.9913:**\n",
    "- which indicates a very strong fit to the training data. The model explains 99.13% of the variance in the closing value, which is excellent.\n",
    "- The predicted values closely align with the actual closing values and hence the plot is accurately showing the trend.\n",
    "  \n",
    "**ElasticNet R² Error on test is = 0.9746:**\n",
    "- In this case  (Alpha = 0.01, L1_ratio = 0.5): Alpha=0.01 means this is a small regularization strength. It allows the model to learn the data well without too much restriction, enabling it to fit both the training and test data effectively.\n",
    "\n",
    "**Conclusion**\n",
    "\n",
    "Linear Regression delivers high performance but risks overfitting due to its lack of regularization.\n",
    "ElasticNet gives a balance between fitting the data well and preventing overfitting with small regularization, making it a reliable model, particularly when the dataset is complex or noisy.\n",
    "\n",
    "Both models are performing excellently, but ElasticNet may provide a safer choice for unseen data where overfitting could be a concern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "91bba91c-5e9d-4d6e-a71f-4637cabcf1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grade 4\n",
    "# Calculating the On-Balance Volume (OBV)\n",
    "min_to_day_df['OBV'] = 0.0\n",
    "for i in range(1, len(min_to_day_df)):\n",
    "    if min_to_day_df['Close'].iloc[i] > min_to_day_df['Close'].iloc[i - 1]:\n",
    "        min_to_day_df.loc[min_to_day_df.index[i], 'OBV'] = min_to_day_df.loc[min_to_day_df.index[i - 1], 'OBV'] + min_to_day_df.loc[min_to_day_df.index[i], 'Volume']\n",
    "    elif min_to_day_df['Close'].iloc[i] < min_to_day_df['Close'].iloc[i - 1]:\n",
    "        min_to_day_df.loc[min_to_day_df.index[i], 'OBV'] = min_to_day_df.loc[min_to_day_df.index[i - 1], 'OBV'] - min_to_day_df.loc[min_to_day_df.index[i], 'Volume']\n",
    "    else:\n",
    "        min_to_day_df.loc[min_to_day_df.index[i], 'OBV'] = min_to_day_df.loc[min_to_day_df.index[i - 1], 'OBV']\n",
    "\n",
    "min_to_day_df['On_Balance_Vol'] = min_to_day_df['OBV']\n",
    "min_to_day_df.drop(columns=['OBV'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2380b209-b40f-455c-8df0-2280e404dbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Defining a create slide window function\n",
    "def create_sliding_windows(data, window_size):\n",
    "    features = data[['Close']].values\n",
    "    labels = data['Label'].values            # Extract labels (1D array)\n",
    "    \n",
    "    X, y = [], []\n",
    "    \n",
    "    for i in range(len(data) - window_size + 1):\n",
    "        # Create windowed data\n",
    "        window = features[i:i + window_size]\n",
    "        \n",
    "        # Reshape the window into a single vector\n",
    "        window_vector = window.flatten()\n",
    "        \n",
    "        # Append the reshaped window and associated label (last label in window)\n",
    "        X.append(window_vector)\n",
    "        y.append(labels[i + window_size - 1])\n",
    "    \n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4605729-eafe-4f17-9046-66b97be90dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "# Loop through polynomial degrees and window sizes\n",
    "results_summary = []\n",
    "\n",
    "# Apply polynomial transformation on training data : No slide window case\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "X_no_window_train = train_final_df[['Close']].values\n",
    "X_poly_no_window_train = poly.fit_transform(X_no_window_train)\n",
    "    \n",
    "# Fit the linear regression model on training data\n",
    "model = LinearRegression()\n",
    "model.fit(X_poly_no_window_train, train_final_df['Label'])\n",
    "\n",
    "# Apply polynomial transformation on test data\n",
    "X_no_window_test = test_final_df[['Close']].values\n",
    "X_poly_no_window_test = poly.transform(X_no_window_test)\n",
    "\n",
    "# Make predictions on test data\n",
    "y_no_window_test_pred = model.predict(X_poly_no_window_test)\n",
    "r2_no_window_test = r2_score(test_final_df['Label'], y_no_window_test_pred)\n",
    "\n",
    "# Append the results for both training and test set in the results summary\n",
    "results_summary.append({\n",
    "    'Model': f'Polynomial Reg Deg 2', \n",
    "    'Window Size': 'No Window', \n",
    "    'R²': r2_no_window_test\n",
    "    })\n",
    "\n",
    "\n",
    "# Loop through polynomial degrees and window sizes \n",
    "for window_size in [2, 5, 10]:\n",
    "    # Create sliding windows for training data\n",
    "    X_train, y_train = create_sliding_windows(train_final_df, window_size)\n",
    "\n",
    "    # Fit the polynomial regression model\n",
    "    poly = PolynomialFeatures(degree=2)\n",
    "    X_poly_train = poly.fit_transform(X_train)\n",
    "\n",
    "    # Fit the linear regression model\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_poly_train, y_train)\n",
    "\n",
    "     # Create sliding windows for testing data\n",
    "    X_test, y_test = create_sliding_windows(test_final_df, window_size)\n",
    "\n",
    "    # Transform test data\n",
    "    X_poly_test = poly.transform(X_test)\n",
    "\n",
    "    # Make predictions on test data\n",
    "    y_test_pred = model.predict(X_poly_test)\n",
    "    \n",
    "    # Calculate R² score\n",
    "    r2 = r2_score(y_test, y_test_pred)\n",
    "\n",
    "    # Collect the results in a list\n",
    "    results_summary.append({\n",
    "    'Model': f'Polynomial Reg Deg 2', \n",
    "        'Window Size': window_size, \n",
    "        'R²': r2\n",
    "        })\n",
    "\n",
    "results_summary_df = pd.DataFrame(results_summary)\n",
    "print(results_summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f28a411-1641-41c4-9710-29ca1e8c9851",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "# Loop through polynomial degrees and window sizes\n",
    "results_summary = []\n",
    "\n",
    "# Defining a function to show results\n",
    "def show_slide_window_results(model_name, window_size, r2):\n",
    "    results_summary.append({\n",
    "        'Model': model_name, \n",
    "        'Window Size': 'No Window' if window_size is None else window_size, \n",
    "        'R²': r2\n",
    "    })\n",
    "\n",
    "# Defining all three models: LinearRegression, ElasticNet and Polynomial Regression with window size: 2,5,10\n",
    "def fit_run_three_models(train_data_df, test_data_df):\n",
    "    for window_size in [None, 2, 5, 10]:\n",
    "        # If window size is None, use the entire dataset without sliding windows\n",
    "        if window_size is None:\n",
    "            X_train = train_data_df[['Close']].values\n",
    "            y_train = train_data_df['Label'].values\n",
    "            X_test = test_data_df[['Close']].values\n",
    "            y_test = test_data_df['Label'].values\n",
    "        else:\n",
    "            # Create sliding windows for training data\n",
    "            X_train, y_train = create_sliding_windows(train_data_df, window_size)\n",
    "\n",
    "            # Create sliding windows for testing data\n",
    "            X_test, y_test = create_sliding_windows(test_data_df, window_size)\n",
    "\n",
    "        # Fit the polynomial regression model\n",
    "        poly = PolynomialFeatures(degree=2)\n",
    "        X_poly_train = poly.fit_transform(X_train)\n",
    "\n",
    "        # Fit the linear regression model\n",
    "        model = LinearRegression()\n",
    "        model.fit(X_poly_train, y_train)\n",
    "\n",
    "        # Transform test data\n",
    "        X_poly_test = poly.transform(X_test)\n",
    "\n",
    "        # Make predictions on test data\n",
    "        y_test_pred = model.predict(X_poly_test)\n",
    "\n",
    "        # Calculate R² score\n",
    "        r2_poly_reg = r2_score(y_test, y_test_pred)\n",
    "\n",
    "        show_slide_window_results('Polynomial Reg', window_size, r2_poly_reg)\n",
    "\n",
    "\n",
    "\n",
    "        # --- Linear Regression ---\n",
    "        # Fit the linear regression model directly on original features\n",
    "        lin_model = LinearRegression()\n",
    "        lin_model.fit(X_train, y_train)\n",
    "\n",
    "        # Make predictions on the test data (linear regression)\n",
    "        y_lin_test_pred = lin_model.predict(X_test)\n",
    "\n",
    "        # Calculate the R² score for linear regression\n",
    "        r2_linear_reg = r2_score(y_test, y_lin_test_pred)\n",
    "\n",
    "        show_slide_window_results('Linear Reg', window_size, r2_linear_reg)\n",
    "\n",
    "\n",
    "       # --- ElasticNet Regression ---\n",
    "        # Fit the ElasticNet model\n",
    "        elastic_net_model = ElasticNet(alpha=0.01, l1_ratio=0.5)  # You can adjust alpha and l1_ratio\n",
    "        elastic_net_model.fit(X_train, y_train)\n",
    "\n",
    "        # Make predictions on the test data (ElasticNet)\n",
    "        y_en_test_pred = elastic_net_model.predict(X_test)\n",
    "\n",
    "        # Calculate the R² score for ElasticNet\n",
    "        r2_elasticNet = r2_score(y_test, y_en_test_pred)\n",
    "\n",
    "        # Append ElasticNet results to summary\n",
    "        show_slide_window_results('ElasticNet', window_size, r2_elasticNet)\n",
    "\n",
    "# Calling the function for fit and run three models with window size and showing the results\n",
    "fit_run_three_models(train_final_df, test_final_df)\n",
    "results_summary_df = pd.DataFrame(results_summary)\n",
    "print(results_summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75a0df1-e0b8-4080-945e-8928a3cdfded",
   "metadata": {},
   "source": [
    "### Grade 4:\n",
    "### 1. Summarize and compare their R² error measures. Is anyone better than the LinearRegression model without window information attached?\n",
    "### Explanation \n",
    "\n",
    "**Comparison Analysis:**\n",
    "- **Polynomial Regression without Window (R² = 0.991263):** performs slightly worse than Linear Regression without window(R² = 0.991294).\n",
    "Among the different window sizes for Linear and Polynomial Regression, **the Linear Regression with a window size of 2 (R² = 0.991309)** performs the best in that group.\n",
    "ElasticNet consistently has lower R² values across all R² values compared to Linear Regression and Polynomial Regression.\n",
    "\n",
    "**Conclusion:**\n",
    "\n",
    "The Linear Regression (Window 2) model has the best performance in terms of R², outperforming all polynomial and ElasticNet models. Therefore, the Linear Regression model (Window 2) is the best option among those tested, followed closely by the **Linear Regression without window \n",
    "(0.991294)** and **Polynomial Regression without Window (R² = 0.991263)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aac0b4c3-d3a7-43cb-8fca-567fff33bd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# defining the function for window wise dataframes with close, label, predicted and difference column for calculating the Hit ratio for each window.\n",
    "def polynomial_regression_with_sliding_windows(train_df, test_df, window_size, degree=2):\n",
    "    # Create sliding windows for training data\n",
    "    X_train, y_train = create_sliding_windows(train_final_df, window_size)\n",
    "\n",
    "    # Fit the polynomial regression model\n",
    "    poly = PolynomialFeatures(degree=degree)\n",
    "    X_poly_train = poly.fit_transform(X_train)\n",
    "\n",
    "    # Fit the linear regression model\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_poly_train, y_train)\n",
    "\n",
    "    # Create sliding windows for testing data\n",
    "    X_test, y_test = create_sliding_windows(test_final_df, window_size)\n",
    "\n",
    "    # Transform test data\n",
    "    X_poly_test = poly.transform(X_test)\n",
    "\n",
    "    # Make predictions on test data\n",
    "    y_test_pred = model.predict(X_poly_test)\n",
    "\n",
    "    # Ensure you have the right index for the predictions\n",
    "    start_index = window_size  # Start index for the actual close values\n",
    "    predicted_length = len(y_test_pred)  # Number of predictions made by the model\n",
    "\n",
    "    # Extract actual close values based on the start index and predicted length\n",
    "    actual_close_values = test_df['Close'].values[start_index:start_index + predicted_length]\n",
    "\n",
    "    # Adjust lengths if they don't match\n",
    "    if len(y_test_pred) != len(actual_close_values):\n",
    "        min_length = min(len(y_test_pred), len(actual_close_values))\n",
    "\n",
    "        # Truncate to the minimum length\n",
    "        y_test_pred = y_test_pred[:min_length]\n",
    "        actual_close_values = actual_close_values[:min_length]\n",
    "        y_test = y_test[:min_length]  # Also adjust y_test for consistency\n",
    "\n",
    "    # Create a DataFrame for test results\n",
    "    results_df = pd.DataFrame({\n",
    "        'Close': actual_close_values,\n",
    "        'Label': y_test,\n",
    "        'Predicted': y_test_pred,\n",
    "    })\n",
    "\n",
    "    # Calculate the difference directly in the DataFrame\n",
    "    results_df['Difference'] = results_df['Predicted'] - results_df['Close']\n",
    "\n",
    "    return results_df\n",
    "\n",
    "# Initialize an empty dictionary to store DataFrames\n",
    "dataframes = {}\n",
    "\n",
    "# Loop through the desired window sizes and create DataFrames\n",
    "for window_size in [2, 5, 10]:\n",
    "    dataframes[window_size] = polynomial_regression_with_sliding_windows(train_data_df, test_data_df, window_size)\n",
    "\n",
    "# Accessing DataFrames for each window size\n",
    "df_window_2 = dataframes[2]\n",
    "df_window_5 = dataframes[5]\n",
    "df_window_10 = dataframes[10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf5b8c1-0099-481c-8828-90708eaca036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grade 5\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "mae = mean_absolute_error(one_day_test_df['Label'], one_day_test_df['Predicted'])\n",
    "print(\"Mean Absolute Error:\", mae)\n",
    "one_day_test_df['Difference'] = one_day_test_df['Predicted'] - one_day_test_df['Close']\n",
    "\n",
    "# Defining a function for decision making based on value difference\n",
    "def make_decision(df):\n",
    "    if 0.00001<= df['Difference'] <= 0.000015:\n",
    "        return 'Hold'\n",
    "    elif df['Difference'] <  0.00001 :\n",
    "        return 'Sell'\n",
    "    else:\n",
    "        return 'Buy'\n",
    "\n",
    "one_day_test_df['Decision'] = one_day_test_df.apply(make_decision, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f70477-0876-433a-87a8-f670c1156b7f",
   "metadata": {},
   "source": [
    "### Grade 5\n",
    "### 1. Compare the regression forecast with the known Close price.\n",
    "### Explanation \n",
    "model’s predictions are off by a very small amount—0.00257—compared to the actual value of next day close price(Label).\n",
    "This indicates the model is predicting very accurately, as the error is quite minimal relative to the actual close price.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "89ccdd84-2831-4d84-bdf8-458df18c8642",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_show_hit_ratio(data_df,forecast_type):\n",
    "    correct_prediction = calculating_correct_prediction(data_df)\n",
    "    #  Get the correct predictions 'True' count for the Hit Ratio\n",
    "    correct_prediction_count = correct_prediction['Correct Prediction'].value_counts().get('True', 0)\n",
    "    total_predictions_count = len(correct_prediction['Decision'])\n",
    "    print(f'{correct_prediction_count} {total_predictions_count}')\n",
    "    print(f'hit ratio for {forecast_type}  is {correct_prediction_count/total_predictions_count: .2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62b705a-0116-498c-880a-b06cca0df87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to calculate the correct prediction\n",
    "def calculating_correct_prediction(data_df):\n",
    "    correct_predictions = []\n",
    "    \n",
    "    for i in range(0, len(data_df)):\n",
    "        decision = data_df.iloc[i]['Decision']\n",
    "        actual_next_day_close = data_df.iloc[i]['Label']  # Actual closing value for the next day\n",
    "        actual_current_close = data_df.iloc[i]['Close']   # Current day's closing value\n",
    "        model_predicted_close = data_df.iloc[i]['Predicted']  # Predicted closing value\n",
    "\n",
    "        # Evaluate predictions based on the decision made\n",
    "        if decision == 'Buy':\n",
    "            if actual_next_day_close > actual_current_close:  # Next day value is higher than current day value\n",
    "                correct_predictions.append('True')\n",
    "            else:\n",
    "                correct_predictions.append('False')\n",
    "        \n",
    "        elif decision == 'Sell':\n",
    "            if actual_next_day_close < actual_current_close:  # Next day close value is lower than current day value\n",
    "                correct_predictions.append('True')\n",
    "            else:\n",
    "                correct_predictions.append('False')\n",
    "        \n",
    "        elif decision == 'Hold':\n",
    "            # Check if the predicted value is close enough to the actual next day's value\n",
    "            difference = abs(model_predicted_close - actual_next_day_close)\n",
    "            if 0.00001 <= difference <= 0.000015:  # Threshold for Hold condition (adjust if needed)\n",
    "                correct_predictions.append('True')\n",
    "            else:\n",
    "                correct_predictions.append('False')\n",
    "    \n",
    "    # Assign the list to the new column in the DataFrame after the loop\n",
    "    data_df['Correct Prediction'] = correct_predictions\n",
    "\n",
    "    # Return the DataFrame (optional)\n",
    "    return data_df\n",
    "\n",
    "# hit ratio for one day Forecast  is\n",
    "calculate_show_hit_ratio(one_day_test_df,'One Day')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e237d8-4f44-4934-8b07-a520b1531684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Hit Ratio (HR) of your investment decision for each of the windows. \n",
    "df_window_2['Decision'] = df_window_2.apply(make_decision, axis=1)\n",
    "df_window_5['Decision'] = df_window_5.apply(make_decision, axis=1)\n",
    "df_window_10['Decision'] = df_window_10.apply(make_decision, axis=1)\n",
    "calculate_show_hit_ratio(df_window_2,'window 2')\n",
    "calculate_show_hit_ratio(df_window_5,'window 5')\n",
    "calculate_show_hit_ratio(df_window_10,'window 10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8f4e55-ccd6-4a02-9724-2eaff3121df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HR_calc_week_df contains the weekly close (label data) and close value for same day and below will split in train and test\n",
    "weekly_train_data_df, weekly_test_data_df = split_data(HR_calc_week_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "10863867-a6ab-4f2c-a716-77df11d79cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the one week ahead train and test data except Label(target column)\n",
    "weekly_train_final, weekly_test_final, scalar = standardized_train_test_data(weekly_train_data_df, weekly_test_data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ff89409e-9d1e-4578-9941-4778556b9e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate the Linear Regression model\n",
    "r2_train , r2_test, week_ahead_actual_predicted , y_test, y_test_pred = train_and_evaluate_model(weekly_train_final, weekly_test_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "812bffa4-097b-495a-aa79-258efd74c8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the index of week_ahead_actual_predicted to align by position\n",
    "week_ahead_actual_predicted.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "90a12d55-0133-4c57-9e6f-90456087fec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the 'Predicted' values to weekly_test_data_df\n",
    "weekly_test_data_df['Predicted'] = week_ahead_actual_predicted['Predicted'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "918cc640-a69d-4445-847d-9df39163058b",
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_test_data_df['Difference'] = weekly_test_data_df['Predicted'] - weekly_test_data_df['Close']\n",
    "weekly_test_data_df['Decision'] = weekly_test_data_df.apply(make_decision, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05319cde-6e54-431c-a309-21ff1c53e55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling the hit ratio function on one week ahead forecast data\n",
    "calculate_show_hit_ratio(weekly_test_data_df, 'weekly Forecast')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3592328-b223-4d6b-9e58-38597ef22f60",
   "metadata": {},
   "source": [
    "### 2. Which setup was the best, and why was that?\n",
    "### Explanation\n",
    "**Hit Ratio:**\n",
    " - **Weekly Forecast: 0.49**\n",
    "    \n",
    " - **One Day Forecast: 0.45**\n",
    "\n",
    "In this case, the weekly forecast has a hit ratio of 0.49, meaning that 49% of its predictions were correct.\n",
    "The one-day forecast has a slightly lower hit ratio of 0.45, indicating that 45% of its predictions were correct.\n",
    "\n",
    "**Conclusion**\n",
    "\n",
    "Based on the provided hit ratios, the weekly forecast setup was the better model for predicting outcomes. It had a higher hit ratio compared to the one-day forecast, suggesting that it produced more accurate predictions over the longer time horizon.\n",
    "\n",
    "However, the sliding window forecasts (window sizes of 2, 5, and 10) had a hit ratio of 0,so this approach failed to make accurate predictions. Thus, focusing on the weekly forecast may be the most reliable strategy based on this analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8947d6ff-d63b-4455-a517-0afbeb206c8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
